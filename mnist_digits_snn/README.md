This module is concerned with generating input MNIST sequences, training and exploring properties of spiking networks. Network implementation is based on <a href=https://github.com/jeshraghian/snntorch>snnTorch</a> (recurrent LIF units, backpropagation, surrogate gradients) and experimentation with different training configurations is powered by <a href=https://github.com/ikharitonov/nnconfigs>nnconfigs</a>.

Inputs are generated in __mnist_sequences.ipynb__. Requires downloaded MNIST in .csv format. Using _Pandas_, digits are collated into sequences starting from any digit in [0,9] range (same as in <a href=https://www.cell.com/patterns/fulltext/S2666-3899(22)00271-9>the original paper</a>). Variables for the digit duration, the period in between digit presentations and, importantly, the sampling frequency (250 Hz mainly used throughout experimentation) are defined there. Generated NumPy files are loaded as PyTorch Datasets in __datasets.py__.

__SNNCustomConfig.py__ and __default_configs__ folder are relevant to _nnconfigs_ library.

The basic architecture of the spiking network consists of a population of recurrently connected snntorch.RLeaky units. At each time step, unweighted inputs are fed into this network while the membrane potential levels and spiking activity are recorded. RLeaky units contain torch.nn.Linear layers embedded within, hence they also have bias parameters, apart from weights. These biases have been excluded from the list of learnable parameters in some variations of the architecutre. Models with synaptic decay are also included (snntorch.RSynaptic units).

Models were mainly trained in unsupervised fashion (by running __snntorch_training.py__ or __input_strength_experiment.py__ files in command line), but a supervised approach has also been tested (see relevant implementations in __models.py__, __datasets.py__ and __supervised_training.py__).

One strategy explored was using the weights from another spiking model <a href=https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_5.html>trained to classify MNIST digits</a>. In __snntorch_classification_tutorial.ipynb__, this network is trained and weights matrix is saved for further use.

Initial testing of different input strength levels (rheobase analysis), i.e. maximum intensities of MNIST digits presented, was implemented in __LIF_fi_curve.ipynb__, where a simple input current pulse of increasing strength was related to the frequency of spiking produced within a single LIF unit. Useful outputs from this analysis included a more informed choice for the minimum input strength which can elicit spiking and the maximum one where the units enter a "persistent spike state" regime (possibly caused by numerical effects). These insights as well as further network training runs (__input_strength_experiment.py__) allowed to define a "working range" of input strengths (see __parameter_space_overview.ipynb__) and better understand when optimisation results in "epileptic" network activity or exploding gradients. For all network configurations trained, __snn_analysis.ipynb__ is the main collection of plots for visualisation and analysis. Some initial grouped results are shown in __training_results.ipynb__ and __20march_nb.ipynb__.

Networks were trained on a GPU, but the RAM-hungry nature of the current implementation is likely the main constraint to watch out for. The first thing to try is decreasing temporal resolution of input sequences, while keeping in mind the effect on membrane time constants and the f-i curve.
